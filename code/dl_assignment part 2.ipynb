{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 2 of the assignment, we switch from text processing to image processing to explore\n",
    "another set of model tasks. Again, you are asked to follow a set of instructions below and report\n",
    "on your findings.\n",
    "For this assignment we will make use of the CIFAR-100 dataset of images. Rather than using all\n",
    "images, you are asked to randomly split CIFAR into two subsets of 50 classes each. Your first\n",
    "task is to make these subsets and save these subsets to disk for use later. Make sure to randomly\n",
    "select these classes (donâ€™t just take the first 50). Also make sure to account for training, validation,\n",
    "or test as appropriate. We will refer to the first set of data as Block 1 and the second set of 50\n",
    "classes as Block 2.\n",
    "As with Part 1, the set of tasks below do not build on each other and can in some ways be\n",
    "considered independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.layers import LeakyReLU, Rescaling, Dense, Reshape, Flatten, Input, BatchNormalization, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_epochs = 50\n",
    "num_classes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the .npz file\n",
    "block1 = np.load(\"block1_data.npz\")\n",
    "# Load the data from the .npz file\n",
    "block2 = np.load(\"block2_data.npz\")\n",
    "\n",
    "\n",
    "# Access the arrays within the .npz file\n",
    "# Example: Assuming the file contains arrays named 'x_train', 'y_train', etc.\n",
    "x_train_block1 = block1['x_train']\n",
    "y_train_block1 = block1['y_train']\n",
    "x_val_block1 = block1['x_val']\n",
    "y_val_block1 = block1['y_val']\n",
    "x_test_block1 = block1['x_test']\n",
    "y_test_block1 = block1['y_test']\n",
    "\n",
    "x_train_block2 = block2['x_train']\n",
    "y_train_block2 = block1['y_train']\n",
    "x_test_block2 = block2['x_test']\n",
    "y_test_block2 = block1['y_test']\n",
    "x_val_block2 = block1['x_val']\n",
    "y_val_block2 = block1['y_val']\n",
    "\n",
    "\n",
    "y_train_block1_one_hot = to_categorical(y_train_block1, num_classes=num_classes)\n",
    "y_val_block1_one_hot = to_categorical(y_val_block1, num_classes=num_classes)\n",
    "y_test_block1_one_hot = to_categorical(y_test_block1, num_classes=num_classes)\n",
    "\n",
    "# Check the shapes of the datasets\n",
    "print(f'Block 1 train shape: {x_train_block1.shape}, Block 1 val shape: {x_val_block1.shape}, Block 1 test shape: {x_test_block1.shape}')\n",
    "print(f'Block 2 train shape: {x_train_block2.shape}, Block 2 test shape: {x_test_block2.shape}')\n",
    "\n",
    "\n",
    "# Function to plot images\n",
    "def plot_images(images, labels, title):\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i in range(5):\n",
    "        ax = plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(f\"Label: {labels[i][0]}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot 5 images from the training set\n",
    "plot_images(x_train_block1, y_train_block1, \"Training Set\")\n",
    "\n",
    "# Plot 5 images from the test set\n",
    "plot_images(x_test_block1, y_test_block1, \"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Network Model\n",
    "def build_model_softmax(input_shape=(32, 32, 3), num_classes=50):\n",
    "    model = models.Sequential([\n",
    "        Rescaling(1.0 / 255.0, input_shape=input_shape),  # Normalize the input\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
    "                  loss='sparse_categorical_crossentropy',  # Sparse Categorical Crossentropy for integer labels\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_model_softmax_activations_tanh(input_shape=(32, 32, 3), num_classes=50):\n",
    "    model = models.Sequential([\n",
    "        Rescaling(1.0 / 255.0, input_shape=input_shape),  # Normalize the input\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='tanh'),\n",
    "        layers.Dense(256, activation='tanh'),\n",
    "        layers.Dense(128, activation='tanh'),\n",
    "        layers.Dense(64, activation='tanh'),\n",
    "        layers.Dense(32, activation='tanh'),\n",
    "        layers.Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
    "                  loss='sparse_categorical_crossentropy',  # Sparse Categorical Crossentropy for integer labels\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Basic Network Model with LeakyReLU    \n",
    "def build_model_with_activations_leakyReLU(input_shape=(32, 32, 3), num_classes=50):\n",
    "    model = models.Sequential([\n",
    "        Rescaling(1.0 / 255.0, input_shape=input_shape),  # Normalize the input\n",
    "        layers.Flatten(),        \n",
    "        layers.Dense(512),\n",
    "        LeakyReLU(), \n",
    "        layers.Dense(256),\n",
    "        LeakyReLU(), \n",
    "        layers.Dense(128),\n",
    "        LeakyReLU(),\n",
    "        layers.Dense(64),\n",
    "        LeakyReLU(),\n",
    "        layers.Dense(32),\n",
    "        LeakyReLU(),\n",
    "        layers.Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Basic Network Model with LeakyReLU    \n",
    "def build_model_with_activations_leakyReLU_cc(input_shape=(32, 32, 3), num_classes=50):\n",
    "    model = models.Sequential([\n",
    "        Rescaling(1.0 / 255.0, input_shape=input_shape),  # Normalize the input\n",
    "        layers.Flatten(),        \n",
    "        layers.Dense(512),\n",
    "        LeakyReLU(), \n",
    "        layers.Dense(256),\n",
    "        LeakyReLU(), \n",
    "        layers.Dense(128),\n",
    "        LeakyReLU(),\n",
    "        layers.Dense(64),\n",
    "        LeakyReLU(),\n",
    "        layers.Dense(32),\n",
    "        LeakyReLU(),\n",
    "        layers.Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# List of model build functions and their names\n",
    "models_to_test = [\n",
    "    (build_model_softmax, \"softmax_sparse_categorical_relu\", False),\n",
    "    (build_model_with_activations_leakyReLU, \"softmax_sparse_categorical_leakyReLU\", False),\n",
    "    (build_model_softmax_activations_tanh, \"softmax_sparse_categorical_tanh\", False),\n",
    "    (build_model_with_activations_leakyReLU_cc, \"softmax_categorical_crossentropy\", True)\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through all models\n",
    "for build_model_fn, model_name, encoding in models_to_test:\n",
    "    print(f\"Training model: {model_name}\")\n",
    "    \n",
    "    # Build and train the model\n",
    "    model = build_model_fn()\n",
    "    model.summary()\n",
    "\n",
    "    # Define the checkpoint callback dynamically using lambda to include the model name\n",
    "    checkpoint_path = f'part_2/best_model_{model_name}.keras'\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )  \n",
    "\n",
    "    # Define early stopping and model checkpoint\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    if encoding:\n",
    "        # Train the model using one-hot encoded labels\n",
    "        history_model = model.fit(\n",
    "            x_train_block1, \n",
    "            y_train_block1_one_hot, \n",
    "            epochs=number_epochs, \n",
    "            validation_data=(x_val_block1, y_val_block1_one_hot), \n",
    "            callbacks=[early_stopping, checkpoint]\n",
    "        )\n",
    "    else:\n",
    "        # Train the model using integer labels\n",
    "        history_model = model.fit(\n",
    "            x_train_block1, \n",
    "            y_train_block1, \n",
    "            epochs=number_epochs, \n",
    "            validation_data=(x_val_block1, y_val_block1), \n",
    "            callbacks=[early_stopping, checkpoint]\n",
    "        )\n",
    "\n",
    "    \n",
    "    # Load the best model saved during training\n",
    "    best_model = load_model(f'part_2/best_model_{model_name}.keras')\n",
    "\n",
    "    # Evaluate the model\n",
    "    if encoding:\n",
    "        test_loss, test_accuracy = best_model.evaluate(x_test_block1, y_test_block1_one_hot)\n",
    "    else:\n",
    "        test_loss, test_accuracy = best_model.evaluate(x_test_block1, y_test_block1)\n",
    "\n",
    "    # Evaluate the model on the test data (Block 1)\n",
    "    # test_loss, test_acc = best_model.evaluate(x_test_block1, y_test_block1)\n",
    "    \n",
    "    # Store the results in the list\n",
    "    results.append({\n",
    "        'model_name': model_name,\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'model_history': history_model,\n",
    "        'model': best_model\n",
    "    })\n",
    "    \n",
    "    print(f\"Test Loss ({model_name}): {test_loss}\")\n",
    "    print(f\"Test Accuracy ({model_name}): {test_accuracy}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Plot training and validation accuracy for the models (using the available history)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loop through each row in the results DataFrame and plot the test accuracy\n",
    "for index, row in results_df.iterrows():\n",
    "    model_name = row['model_name']\n",
    "    model_history = row['model_history']\n",
    "    \n",
    "    plt.plot(model_history.history['accuracy'], label=f'{model_name} Training Accuracy')\n",
    "    plt.plot(model_history.history['val_accuracy'], label=f'{model_name} Validation Accuracy')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.title('Training and Validation Accuracy for All Models')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming results_df is your DataFrame with columns 'model_name' and 'test_accuracy'\n",
    "\n",
    "# Extract the relevant data\n",
    "model_names = results_df['model_name']\n",
    "test_accuracies = results_df['test_accuracy']\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "plot_df = pd.DataFrame({\n",
    "    'model_name': model_names,\n",
    "    'test_accuracy': test_accuracies\n",
    "})\n",
    "\n",
    "# Plot test_accuracy per model_name\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='model_name', y='test_accuracy', data=plot_df)\n",
    "plt.title('Test Accuracy per Model')\n",
    "plt.xlabel('Model Name')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.ylim(0, 1)  # Set y-axis limits from 0 to 1 for better visualization\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize the images between 0 and 1\n",
    "x_train_block1_norm = x_train_block1.astype('float32') / 255.0\n",
    "x_val_block1_norm = x_val_block1.astype('float32') / 255.0\n",
    "x_test_block1_norm = x_test_block1.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = results_df.loc[results_df['model_name'] == 'softmax_sparse_categorical_leakyReLU']\n",
    "base_model_auto_input= row['model'].values[0]\n",
    "\n",
    "# Define the Autoencoder Model\n",
    "def build_autoencoder(input_shape=(32, 32, 3), latent_dim=50):\n",
    "    # Encoder\n",
    "    input_img = layers.Input(shape=input_shape)\n",
    "    x = layers.Flatten()(input_img)  # Flatten the image\n",
    "    x = layers.Dense(512)(x)  # Reduce dimensionality\n",
    "    x = layers.LeakyReLU()(x)  # Use LeakyReLU for better gradient flow\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dense(128)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    \n",
    "    # Latent space (bottleneck)\n",
    "    latent_space = layers.Dense(latent_dim)(x)  # Compressed representation\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.Dense(128)(latent_space)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dense(512)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dense(np.prod(input_shape), activation='sigmoid')(x)  # Output layer with sigmoid for [0, 1] range\n",
    "    \n",
    "    # Reshape to the original image size\n",
    "    decoded_img = layers.Reshape(input_shape)(x)\n",
    "    \n",
    "    # Autoencoder model\n",
    "    autoencoder = models.Model(input_img, decoded_img)\n",
    "    \n",
    "    # Compile the model with a smaller learning rate\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=1e-4), loss='mean_squared_error')\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "# Build the Autoencoder\n",
    "autoencoder = build_autoencoder(input_shape=(32, 32, 3), latent_dim=50)\n",
    "\n",
    "# Summary of the model\n",
    "autoencoder.summary()\n",
    "\n",
    "# Define early stopping and model checkpoint\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Callback to save the best model based on validation loss\n",
    "checkpoint = ModelCheckpoint('part_2/best_autoencoder.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the Autoencoder\n",
    "history_autoencoder = autoencoder.fit(\n",
    "    x_train_block1_norm, \n",
    "    x_train_block1_norm,  # Autoencoders train on the input data itself (input == output)\n",
    "    epochs=number_epochs, \n",
    "    validation_data=(x_val_block1_norm, x_val_block1_norm),\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate the autoencoder on test data\n",
    "test_loss_autoencoder = autoencoder.evaluate(x_test_block1_norm, x_test_block1_norm)\n",
    "\n",
    "print(f'Test Loss (Autoencoder): {test_loss_autoencoder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model saved during training\n",
    "best_autoencoder = load_model('part_2/best_autoencoder.keras')\n",
    "\n",
    "results_df\n",
    "\n",
    "# Get the reconstructed images from the autoencoder\n",
    "reconstructed_images = best_autoencoder.predict(x_test_block1_norm)\n",
    "\n",
    "# Plot original vs. reconstructed images\n",
    "n = 10  # Number of images to display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Original image\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test_block1_norm[i])\n",
    "    plt.gray()\n",
    "    ax.set_title(\"Original\")\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Reconstructed image\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(reconstructed_images[i])\n",
    "    plt.gray()\n",
    "    ax.set_title(\"Reconstructed\")\n",
    "    ax.axis('off')  \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `results_df` contains the base model in `row['model']`\n",
    "row = results_df.loc[results_df['model_name'] == 'softmax_sparse_categorical_leakyReLU']\n",
    "base_model_test = row['model'].values[0]\n",
    "\n",
    "# Freeze the entire model to avoid updating weights during training\n",
    "base_model_test.trainable = False\n",
    "\n",
    "# Reuse all layers except the last one\n",
    "reuse_layers = base_model_test.layers[0:-1]\n",
    "\n",
    "# Define new layers to add after the reused layers\n",
    "new_layers = [\n",
    "    layers.Dense(32, activation='relu'),  # New custom layer\n",
    "    layers.Dense(num_classes, activation='softmax')  # Output layer for multi-class classification\n",
    "]\n",
    "\n",
    "# Create the new transfer model by combining reused layers and new layers\n",
    "transfer_model = models.Sequential(reuse_layers + new_layers, name='transfer_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint('part_2/best_transfer_model.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Compile the model for training\n",
    "transfer_model.compile(optimizer=Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the transfer model to check the structure\n",
    "transfer_model.summary()\n",
    "\n",
    "# Train the model on Block 2 data (only the new layers will be trained)\n",
    "history_transfer_model = transfer_model.fit(x_train_block2, y_train_block2, \n",
    "                                             epochs=number_epochs, \n",
    "                                             validation_data=(x_val_block2, y_val_block2),\n",
    "                                             batch_size=32, \n",
    "                                             callbacks=[checkpoint])\n",
    "\n",
    "# Evaluate the model on Block 2 test data\n",
    "test_loss, test_accuracy = transfer_model.evaluate(x_test_block2, y_test_block2)\n",
    "\n",
    "# Print the test results\n",
    "print(f\"Test Loss (Transfer Learning on Block 1): {test_loss}\")\n",
    "print(f\"Test Accuracy (Transfer Learning on Block 1): {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input, Model\n",
    "import numpy as np\n",
    "\n",
    "# Assuming base_model_test is already loaded from the results_df as the pre-trained model\n",
    "row = results_df.loc[results_df['model_name'] == 'softmax_sparse_categorical_relu']\n",
    "base_model_test = row['model'].values[0]\n",
    "\n",
    "# Define the custom decoder function\n",
    "def create_decoder(latent_dim=50,\n",
    "                   activation='relu',\n",
    "                   final_activation='sigmoid',\n",
    "                   output_shape=(32, 32, 3)):\n",
    "    # Input latent representation\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    # First hidden layer\n",
    "    dense_one = Dense(32, activation=activation)(latent_inputs)\n",
    "    # Second hidden layer\n",
    "    dense_two = Dense(64, activation=activation)(dense_one)\n",
    "    # Third hidden layer\n",
    "    dense_three = Dense(128, activation=activation)(dense_two)\n",
    "    # Fourth hidden layer\n",
    "    dense_four = Dense(256, activation=activation)(dense_three)\n",
    "    # Fifth hidden layer\n",
    "    dense_five = Dense(512, activation=activation)(dense_four)\n",
    "    # Output layer (reshaping back to original input shape)\n",
    "    output_layer = Dense(output_shape[0] * output_shape[1] * output_shape[2],\n",
    "                         activation=final_activation)(dense_five)\n",
    "    outputs = Reshape(output_shape)(output_layer)\n",
    "    decoder = Model(latent_inputs, outputs)\n",
    "    return decoder\n",
    "\n",
    "# Define the Autoencoder Model\n",
    "def build_autoencoder(input_shape=(32, 32, 3), latent_dim=50):\n",
    "    # Encoder: Use the pre-trained base model directly (without the final classification layer)\n",
    "    encoder_input = layers.Input(shape=input_shape)  # Define the input layer for the encoder\n",
    "    encoder_output = base_model_test(encoder_input)  # Apply the base model to the input\n",
    "    \n",
    "    # Add latent space (bottleneck layer)\n",
    "    latent_space = layers.Dense(latent_dim)(encoder_output)  # Compressed representation\n",
    "    \n",
    "    # Create the decoder\n",
    "    decoder = create_decoder(latent_dim=latent_dim, activation='relu', final_activation='sigmoid', output_shape=input_shape)\n",
    "    \n",
    "    # Autoencoder model (encoder + decoder)\n",
    "    autoencoder = models.Model(encoder_input, decoder(latent_space))\n",
    "    \n",
    "    # Compile the model with MSE loss for autoencoder\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=1e-3), loss='mean_squared_error')  # Use MSE for autoencoder\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Build the Autoencoder with pre-trained encoder and custom decoder\n",
    "autoencoder = build_autoencoder(input_shape=(32, 32, 3), latent_dim=50)\n",
    "\n",
    "# Summary of the model to check the structure\n",
    "autoencoder.summary()\n",
    "\n",
    "# Define early stopping and model checkpoint for training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('part_2/from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input, Model\n",
    "import numpy as np\n",
    "\n",
    "# Assuming base_model_test is already loaded from the results_df as the pre-trained model\n",
    "row = results_df.loc[results_df['model_name'] == 'softmax_sparse_categorical_relu']\n",
    "base_model_test = row['model'].values[0]\n",
    "\n",
    "# Define the custom decoder function\n",
    "def create_decoder(latent_dim=50,\n",
    "                   activation='relu',\n",
    "                   final_activation='sigmoid',\n",
    "                   output_shape=(32, 32, 3)):\n",
    "    # Input latent representation\n",
    "    latent_inputs = Input(shape=(latent_dim,))\n",
    "    # First hidden layer\n",
    "    dense_one = Dense(32, activation=activation)(latent_inputs)\n",
    "    # Second hidden layer\n",
    "    dense_two = Dense(64, activation=activation)(dense_one)\n",
    "    # Third hidden layer\n",
    "    dense_three = Dense(128, activation=activation)(dense_two)\n",
    "    # Fourth hidden layer\n",
    "    dense_four = Dense(256, activation=activation)(dense_three)\n",
    "    # Fifth hidden layer\n",
    "    dense_five = Dense(512, activation=activation)(dense_four)\n",
    "    # Output layer (reshaping back to original input shape)\n",
    "    output_layer = Dense(output_shape[0] * output_shape[1] * output_shape[2],\n",
    "                         activation=final_activation)(dense_five)\n",
    "    outputs = Reshape(output_shape)(output_layer)\n",
    "    decoder = Model(latent_inputs, outputs)\n",
    "    return decoder\n",
    "\n",
    "# Define the Autoencoder Model\n",
    "def build_autoencoder(input_shape=(32, 32, 3), latent_dim=50):\n",
    "    # Encoder: Use the pre-trained base model directly (without the final classification layer)\n",
    "    encoder_input = layers.Input(shape=input_shape)  # Define the input layer for the encoder\n",
    "    encoder_output = base_model_test(encoder_input)  # Apply the base model to the input\n",
    "    \n",
    "    # Add latent space (bottleneck layer)\n",
    "    latent_space = layers.Dense(latent_dim)(encoder_output)  # Compressed representation\n",
    "    \n",
    "    # Create the decoder\n",
    "    decoder = create_decoder(latent_dim=latent_dim, activation='relu', final_activation='sigmoid', output_shape=input_shape)\n",
    "    \n",
    "    # Autoencoder model (encoder + decoder)\n",
    "    autoencoder = models.Model(encoder_input, decoder(latent_space))\n",
    "    \n",
    "    # Compile the model with MSE loss for autoencoder\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=1e-3), loss='mean_squared_error')  # Use MSE for autoencoder\n",
    "    \n",
    "    return autoencoder\n",
    "\n",
    "# Build the Autoencoder with pre-trained encoder and custom decoder\n",
    "autoencoder = build_autoencoder(input_shape=(32, 32, 3), latent_dim=50)\n",
    "\n",
    "# Summary of the model to check the structure\n",
    "autoencoder.summary()\n",
    "\n",
    "# Define early stopping and model checkpoint for training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('part_2/best_autoencoder.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the Autoencoder on Block 1 data (input == output)\n",
    "history_autoencoder = autoencoder.fit(\n",
    "    x_train_block1_norm, \n",
    "    x_train_block1_norm,  # Autoencoders train on the input data itself (input == output)\n",
    "    epochs=10,  # Start with a small number of epochs to check training behavior\n",
    "    validation_data=(x_val_block1_norm, x_val_block1_norm),\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate the autoencoder on test data\n",
    "test_loss_autoencoder = autoencoder.evaluate(x_test_block1_norm, x_test_block1_norm)\n",
    "\n",
    "print(f'Test Loss (Autoencoder): {test_loss_autoencoder}')\n",
    ".keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "# Train the Autoencoder on Block 1 data (input == output)\n",
    "history_autoencoder = autoencoder.fit(\n",
    "    x_train_block1_norm, \n",
    "    x_train_block1_norm,  # Autoencoders train on the input data itself (input == output)\n",
    "    epochs=10,  # Start with a small number of epochs to check training behavior\n",
    "    validation_data=(x_val_block1_norm, x_val_block1_norm),\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "# Evaluate the autoencoder on test data\n",
    "test_loss_autoencoder = autoencoder.evaluate(x_test_block1_norm, x_test_block1_norm)\n",
    "\n",
    "print(f'Test Loss (Autoencoder): {test_loss_autoencoder}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model saved during training\n",
    "best_autoencoder = load_model('part_2/best_autoencoder.keras')\n",
    "\n",
    "results_df\n",
    "\n",
    "# Get the reconstructed images from the autoencoder\n",
    "reconstructed_images = best_autoencoder.predict(x_test_block1_norm)\n",
    "\n",
    "# Plot original vs. reconstructed images\n",
    "n = 10  # Number of images to display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Original image\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test_block1_norm[i])\n",
    "    plt.gray()\n",
    "    ax.set_title(\"Original\")\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Reconstructed image\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(reconstructed_images[i])\n",
    "    plt.gray()\n",
    "    ax.set_title(\"Reconstructed\")\n",
    "    ax.axis('off')  \n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
