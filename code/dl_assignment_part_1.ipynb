{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of this project is based around a simple task -- performing genre analysis on the “Multi-\n",
    "Lingual Lyrics for Genre Classification” dataset on kaggle.\n",
    "https://www.kaggle.com/datasets/mateibejan/multilingual-lyrics-for-genre-classification\n",
    "This is an extensive dataset that is split into training and testing subsets. The testing dataset\n",
    "should be used for final testing only. The training dataset should be all training and validation\n",
    "tasks as appropriate.\n",
    "This first part of the task is to perform a number of analyses based on training from scratch to\n",
    "predict genre based on initially on song lyrics and then on song lyrics and artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN, Dropout, Input, Reshape, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Dropout, Flatten\n",
    "from IPython.display import Image\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "# List physical devices to see if a GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to the train and test CSV files\n",
    "train_file_path = 'part_1_data/train_encoded_lemmatize.csv'\n",
    "test_file_path = 'part_1_data/test_encoded_lemmatize.csv'\n",
    "\n",
    "# Load the train and test datasets\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "\n",
    "# Convert all entries in the DataFrames to strings\n",
    "train_df = train_df.astype(str)\n",
    "test_df = test_df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['genre'] = label_encoder.fit_transform(train_df['genre'])\n",
    "test_df['genre'] = label_encoder.transform(test_df['genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1000\n",
    "vocab_size = 10000\n",
    "# Function to preprocess and optionally pad text data\n",
    "def preprocess_text_data(train_df, test_df, max_artist_length=3, use_padding=True):\n",
    "    global max_length\n",
    "    global vocab_size\n",
    "    # Split the training data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_df['lyrics'], train_df['genre'], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Tokenize the text data\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    # vocab_size = len(tokenizer.word_index) + 1  # +1 because word_index starts from 1\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(test_df['lyrics'])\n",
    "\n",
    "    # max_length = max([len(seq) for seq in X_train_seq])\n",
    "    # print(f\"Maximum sequence length: {max_length}\")\n",
    "\n",
    "    # Optionally Pad the sequences\n",
    "    if use_padding:\n",
    "        X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "        X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')\n",
    "        X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "    else:\n",
    "        X_train_pad, X_val_pad, X_test_pad = X_train_seq, X_val_seq, X_test_seq\n",
    "\n",
    "    # Prepare artist data\n",
    "    artist_tokenizer = Tokenizer()\n",
    "    artist_tokenizer.fit_on_texts(train_df['artist'])\n",
    "    X_train_artist_seq = artist_tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_artist_seq = artist_tokenizer.texts_to_sequences(X_val)\n",
    "    X_test_artist_seq = artist_tokenizer.texts_to_sequences(test_df['artist'])\n",
    "\n",
    "    # Optionally Pad the artist sequences\n",
    "    if use_padding:\n",
    "        X_train_artist_pad = pad_sequences(X_train_artist_seq, maxlen=max_artist_length, padding='post')\n",
    "        X_val_artist_pad = pad_sequences(X_val_artist_seq, maxlen=max_artist_length, padding='post')\n",
    "        X_test_artist_pad = pad_sequences(X_test_artist_seq, maxlen=max_artist_length, padding='post')\n",
    "    else:\n",
    "        X_train_artist_pad, X_val_artist_pad, X_test_artist_pad = X_train_artist_seq, X_val_artist_seq, X_test_artist_seq\n",
    "\n",
    "    return X_train_pad, X_val_pad, X_test_pad, X_train_artist_pad, X_val_artist_pad, X_test_artist_pad, y_train, y_val\n",
    "\n",
    "# Example Usage\n",
    "X_train_pad, X_val_pad, X_test_pad, X_train_artist_pad, X_val_artist_pad, X_test_artist_pad, y_train, y_val = preprocess_text_data(\n",
    "    train_df, test_df, max_artist_length=3, use_padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Maximum sequence length: {max_length}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn't exist\n",
    "os.makedirs('part_1', exist_ok=True)\n",
    "\n",
    "print(\"Shape of input data:\", X_train_pad.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.5\n",
    "# Define the model architecture\n",
    "def create_rnn_model(input_length, vocab_size, embedding_dim, state_size, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "    model.add(SimpleRNN(state_size, return_sequences=False, activation=\"tanh\"))\n",
    "    model.add(Dropout(dropout_rate))  # Dropout after the RNN layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_multi_layer_rnn_model(input_length, vocab_size, embedding_dim, state_size, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "    model.add(SimpleRNN(state_size, return_sequences=True))  # First RNN layer with return_sequences=True\n",
    "    model.add(Dropout(dropout_rate))  # Add dropout\n",
    "    model.add(SimpleRNN(state_size, return_sequences=True))  # Second RNN layer with return_sequences=True\n",
    "    model.add(Dropout(dropout_rate))  # Add dropout\n",
    "    model.add(SimpleRNN(state_size))  # Third RNN layer without return_sequences\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(input_length, vocab_size, embedding_dim, state_size, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "    model.add(LSTM(state_size, return_sequences=False, activation=\"tanh\"))\n",
    "    model.add(Dropout(dropout_rate))  # Add dropout\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def create_multi_layer_lstm_model(input_length, vocab_size, embedding_dim, state_size, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
    "    model.add(LSTM(state_size, return_sequences=True, activation=\"tanh\"))\n",
    "    model.add(Dropout(dropout_rate))  \n",
    "    model.add(LSTM(state_size, return_sequences=True, activation=\"tanh\"))\n",
    "    model.add(Dropout(dropout_rate))  \n",
    "    model.add(LSTM(state_size, return_sequences=False, activation=\"tanh\"))\n",
    "    model.add(Dropout(dropout_rate))  \n",
    "    model.add(Dense(num_classes, activation='softmax'))    \n",
    "    return model\n",
    "\n",
    "def create_lstm_model_multi_label(max_length, vocab_size, embedding_dim, state_size, num_classes, artist_vocab_size):\n",
    "    # Input layer for lyrics\n",
    "    lyrics_input = Input(shape=(max_length,), name='lyrics_input')\n",
    "    lyrics_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length)(lyrics_input)\n",
    "    lyrics_lstm = LSTM(state_size, return_sequences=False)(lyrics_embedding)\n",
    "\n",
    "    # Input layer for artist\n",
    "    artist_input = Input(shape=(1,), name='artist_input')\n",
    "    artist_embedding = Embedding(input_dim=artist_vocab_size, output_dim=embedding_dim, input_length=1)(artist_input)\n",
    "    # artist_flat = Flatten()(artist_embedding)\n",
    "    artist_lstm = LSTM(state_size, return_sequences=False)(artist_embedding)\n",
    "\n",
    "    # Concatenate lyrics and artist representations\n",
    "    concatenated = Concatenate()([lyrics_lstm, artist_lstm])\n",
    "    dense = Dense(128, activation='relu')(concatenated)\n",
    "    dropout = Dropout(dropout_rate)(dense)\n",
    "    output = Dense(num_classes, activation='softmax')(dropout)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[lyrics_input, artist_input], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "num_epochs = 10 # Define the number of epochs\n",
    "embedding_dim = 100\n",
    "state_size = 64  # Define the state size for both RNN and LSTM models\n",
    "num_classes = len(label_encoder.classes_)\n",
    "artist_vocab_size = len(train_df['artist'].unique())\n",
    "learning_rate=1e-3\n",
    "\n",
    "\n",
    "# Ensure the directory exists\n",
    "output_dir = 'model_plots'\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "log_dir = \"logs\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "# Create and compile the RNN model\n",
    "rnn_model = create_rnn_model(max_length, vocab_size, embedding_dim, state_size, num_classes)\n",
    "rnn_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create and compile the Multi Layer RNN model\n",
    "multi_layer_rnn_model = create_multi_layer_rnn_model(max_length, vocab_size, embedding_dim, state_size, num_classes)\n",
    "multi_layer_rnn_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Create and compile the LSTM model\n",
    "lstm_model = create_lstm_model(max_length, vocab_size, embedding_dim, state_size, num_classes)\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create and compile the Multi Layer LSTM model\n",
    "multi_layer_lstm_model = create_multi_layer_lstm_model(max_length, vocab_size, embedding_dim, state_size, num_classes)\n",
    "multi_layer_lstm_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Create and compile the LSTM model with multi label input\n",
    "lstm_model_multi = create_lstm_model_multi_label(max_length, vocab_size, embedding_dim, state_size, num_classes, artist_vocab_size)\n",
    "lstm_model_multi.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Compile the model\n",
    "# lstm_model_pretrained = create_pretrained_lstm_model(state_size)\n",
    "# lstm_model_pretrained.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the RNN model\n",
    "num_epochs = 15\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define the checkpoint callback dynamically using lambda to include the model name\n",
    "checkpoint_path = f'part_1/best_rnn_model.keras'\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "  \n",
    "\n",
    "history_rnn = rnn_model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val), epochs=num_epochs, batch_size=32, callbacks=[early_stopping, checkpoint, tensorboard_callback])\n",
    "rnn_model.summary()\n",
    "\n",
    "# Load the best model saved during training\n",
    "best_model = load_model(f'part_1/best_rnn_model.keras')\n",
    "\n",
    "# Evaluate the model on the test data (Block 1)\n",
    "test_loss, test_acc = best_model.evaluate(X_test_pad, test_df['genre'])\n",
    "\n",
    "# Store the results in the list\n",
    "results.append({\n",
    "    'model_name': 'best_rnn_model',\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'model_history': history_rnn\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the stacked RNN model\n",
    "num_epochs = 15\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define the checkpoint callback dynamically using lambda to include the model name\n",
    "checkpoint_path = f'part_1/best_multi_layer_rnn_model.keras'\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "history_multi_layer_rnn = multi_layer_rnn_model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val), epochs=num_epochs, batch_size=32, callbacks=[early_stopping, checkpoint, tensorboard_callback])\n",
    "multi_layer_rnn_model.summary()\n",
    "\n",
    "\n",
    "# Load the best model saved during training\n",
    "best_model = load_model(f'part_1/best_multi_layer_rnn_model.keras')\n",
    "\n",
    "# Evaluate the model on the test data (Block 1)\n",
    "test_loss, test_acc = best_model.evaluate(X_test_pad, test_df['genre'])\n",
    "\n",
    "# Store the results in the list\n",
    "results.append({\n",
    "    'model_name': 'best_multi_layer_rnn_model',\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'model_history': history_multi_layer_rnn\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM model\n",
    "num_epochs = 15\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define the checkpoint callback dynamically using lambda to include the model name\n",
    "checkpoint_path = f'part_1/best_lstm_model.keras'\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "history_lstm = lstm_model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val), epochs=num_epochs, batch_size=32, callbacks=[early_stopping, checkpoint, tensorboard_callback])\n",
    "lstm_model.summary()\n",
    "\n",
    "# Load the best model saved during training\n",
    "best_model = load_model(f'part_1/best_lstm_model.keras')\n",
    "\n",
    "# Evaluate the model on the test data (Block 1)\n",
    "test_loss, test_acc = best_model.evaluate(X_test_pad, test_df['genre'])\n",
    "\n",
    "# Store the results in the list\n",
    "results.append({\n",
    "    'model_name': 'best_lstm_model',\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'model_history': history_lstm\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Multi Layer LSTM model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define the checkpoint callback dynamically using lambda to include the model name\n",
    "checkpoint_path = f'part_1/best_multi_layer_lstm_model.keras'\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "history_multi_layer_lstm = multi_layer_lstm_model.fit(X_train_pad, y_train, validation_data=(X_val_pad, y_val), epochs=num_epochs, batch_size=32, callbacks=[early_stopping, checkpoint, tensorboard_callback])\n",
    "multi_layer_lstm_model.summary()\n",
    "\n",
    "\n",
    "# Load the best model saved during training\n",
    "best_model = load_model(f'part_1/best_multi_layer_lstm_model.keras')\n",
    "\n",
    "# Evaluate the model on the test data (Block 1)\n",
    "test_loss, test_acc = best_model.evaluate(X_test_pad, test_df['genre'])\n",
    "\n",
    "# Store the results in the list\n",
    "results.append({\n",
    "    'model_name': 'best_multi_layer_lstm_model',\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'model_history': history_multi_layer_lstm\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Multi Layer LSTM model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define the checkpoint callback dynamically using lambda to include the model name\n",
    "checkpoint_path = f'part_1/best_lstm_multi_label.keras'\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "history_lstm_multi_label = lstm_model_multi.fit(\n",
    "    [X_train_pad, X_train_artist_pad], y_train,\n",
    "    validation_data=([X_val_pad, X_val_artist_pad], y_val),\n",
    "    epochs=num_epochs,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, checkpoint, tensorboard_callback]\n",
    ")\n",
    "lstm_model_multi.summary()\n",
    "\n",
    "# Load the best model saved during training\n",
    "best_model = load_model(f'part_1/best_lstm_multi_label.keras')\n",
    "\n",
    "# Evaluate the model on the test data (Block 1)\n",
    "test_loss, test_acc = best_model.evaluate([X_test_pad, X_test_artist_pad], test_df['genre'])\n",
    "\n",
    "# Store the results in the list\n",
    "results.append({\n",
    "    'model_name': 'best_lstm_multi_label',\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'model_history': history_lstm_multi_label\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the results array to a file\n",
    "with open('results_v1.pkl', 'wb') as file:\n",
    "    pickle.dump(results, file)\n",
    "print(\"Results array saved to 'results_v1.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the results array from a file\n",
    "with open('results_v1.pkl', 'rb') as file:\n",
    "    loaded_results = pickle.load(file)\n",
    "print(\"Results array loaded from 'results_v1.pkl'.\")\n",
    "\n",
    "# Verify the loaded results\n",
    "print(loaded_results)\n",
    "\n",
    "\n",
    "\n",
    "# Print test accuracies dynamically\n",
    "for result in loaded_results:\n",
    "    print(f\"{result['model_name']} Test Accuracy: {result['test_accuracy']}\")\n",
    "\n",
    "# Plot training and validation accuracy dynamically\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in loaded_results:\n",
    "    model_name = result['model_name']\n",
    "    history = result['model_history']\n",
    "    plt.plot(history.history['accuracy'], label=f'{model_name} Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label=f'{model_name} Validation Accuracy')\n",
    "\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the results array from a file\n",
    "with open('results_v1.pkl', 'rb') as file:\n",
    "    loaded_results = pickle.load(file)\n",
    "print(\"Results array loaded from 'results_v1.pkl'.\")\n",
    "\n",
    "# Print test accuracies dynamically\n",
    "for result in loaded_results:\n",
    "    print(f\"{result['model_name']} Test Accuracy: {result['test_accuracy']}, Test Loss: {result['test_loss']}\")\n",
    "\n",
    "# Prepare data for bar plots\n",
    "model_names = [result['model_name'] for result in loaded_results]\n",
    "test_accuracies = [result['test_accuracy'] for result in loaded_results]\n",
    "test_losses = [result['test_loss'] for result in loaded_results]\n",
    "\n",
    "# Plot Test Accuracy Bar Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(model_names, test_accuracies, color='green')\n",
    "plt.title('Test Accuracy per Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate model names for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of x-axis labels\n",
    "plt.show()\n",
    "\n",
    "# Plot Test Loss Bar Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(model_names, test_losses, color='red')\n",
    "plt.title('Test Loss per Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate model names for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of x-axis labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models on the test set\n",
    "rnn_test_loss, rnn_test_acc = rnn_model.evaluate(X_test_pad, test_df['genre'])\n",
    "lstm_test_loss, lstm_test_acc = lstm_model.evaluate(X_test_pad, test_df['genre'])\n",
    "multi_layer_rnn_test_loss, multi_layer_rnn_test_acc = stacked_rnn_model.evaluate(X_test_pad, test_df['genre'])\n",
    "lstm_multi_test_loss, lstm_multi_test_acc = lstm_model_multi.evaluate([X_test_pad, X_test_artist_pad], test_df['genre'])\n",
    "multi_layer_lstm_test_loss, multi_layer_lstm_test_acc = multi_layer_lstm_model.evaluate(X_test_pad, test_df['genre'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the padded sequences to fit the classifier\n",
    "X_train_flat = X_train_pad.reshape(X_train_pad.shape[0], -1)\n",
    "X_val_flat = X_val_pad.reshape(X_val_pad.shape[0], -1)\n",
    "X_test_flat = X_test_pad.reshape(X_test_pad.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DataFrame to store evaluation results\n",
    "results_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models with their names and initialized instances\n",
    "models = [\n",
    "    (\"Random Forest\", RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "]\n",
    "\n",
    "# Iterate through each model, train, evaluate, and save\n",
    "for idx, (model_name, model_instance) in enumerate(models):\n",
    "    print(f\"Training and evaluating model: {model_name}\")\n",
    "\n",
    "    # Create a pipeline (can be extended for preprocessing if needed)\n",
    "    pipeline = Pipeline([\n",
    "        (model_name, model_instance)\n",
    "    ])\n",
    "    \n",
    "    # Train the pipeline on the training dataset\n",
    "    pipeline.fit(X_train_flat, y_train)\n",
    "    \n",
    "    # Save the trained pipeline\n",
    "    joblib.dump(pipeline, f'{model_name}_pipeline.pkl')\n",
    "    print(f\"Saved {model_name} pipeline to disk.\")\n",
    "    \n",
    "    # Make predictions on the test dataset\n",
    "    y_val_pred = pipeline.predict(X_val_flat)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    # Evaluate the model on the validation set\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred, average='weighted')\n",
    "    recall = recall_score(y_val, y_val_pred, average='weighted')\n",
    "    f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "    \n",
    "    # Add results to the DataFrame using .loc\n",
    "    results_df.loc[idx] = [model_name, accuracy, precision, recall, f1]\n",
    "\n",
    "    # Display predictions (optional, can be removed for large datasets)\n",
    "    print(f\"Predictions for {model_name}: {y_val_pred}\\n\")\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_test_pred = pipeline.predict(X_test_flat)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_accuracy = accuracy_score(test_df['genre'], y_test_pred)\n",
    "    test_precision = precision_score(test_df['genre'], y_test_pred, average='weighted')\n",
    "    test_recall = recall_score(test_df['genre'], y_test_pred, average='weighted')\n",
    "    test_f1 = f1_score(test_df['genre'], y_test_pred, average='weighted')\n",
    "\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print(f\"Test Precision: {test_precision}\")\n",
    "    print(f\"Test Recall: {test_recall}\")\n",
    "    print(f\"Test F1 Score: {test_f1}\")\n",
    "\n",
    "# Display all results in a structured format\n",
    "print(\"Evaluation Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# Save results to a CSV file for future reference\n",
    "results_df.to_csv(\"model_evaluation_results.csv\", index=False)\n",
    "print(\"Saved evaluation results to 'model_evaluation_results.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
