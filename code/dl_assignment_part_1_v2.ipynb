{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of this project is based around a simple task -- performing genre analysis on the “Multi-\n",
    "Lingual Lyrics for Genre Classification” dataset on kaggle.\n",
    "https://www.kaggle.com/datasets/mateibejan/multilingual-lyrics-for-genre-classification\n",
    "This is an extensive dataset that is split into training and testing subsets. The testing dataset\n",
    "should be used for final testing only. The training dataset should be all training and validation\n",
    "tasks as appropriate.\n",
    "This first part of the task is to perform a number of analyses based on training from scratch to\n",
    "predict genre based on initially on song lyrics and then on song lyrics and artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_df = pd.read_csv('train.csv')  # Replace with the path to your train.csv\n",
    "test_df = pd.read_csv('test.csv')  # Replace with the path to your test.csv\n",
    "\n",
    "# Show the first few rows of the train dataset to understand the structure\n",
    "train_df.head()\n",
    "\n",
    "# Convert all entries in the DataFrames to strings\n",
    "train_df = train_df.astype(str)\n",
    "test_df = test_df.astype(str)\n",
    "\n",
    "# Lowercase all column names in the testing dataset\n",
    "test_df.columns = [col.lower() for col in test_df.columns]\n",
    "\n",
    "# Lowercase all column names in the testing dataset\n",
    "train_df.columns = [col.lower() for col in train_df.columns]\n",
    "\n",
    "# Split the training dataset into train and validation sets (80% train, 20% validation)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess text data (convert lyrics to lowercase)\n",
    "train_texts = train_df['lyrics'].str.lower().values\n",
    "val_texts = val_df['lyrics'].str.lower().values\n",
    "test_texts = test_df['lyrics'].str.lower().values\n",
    "\n",
    "# Tokenize the lyrics\n",
    "max_words = 10000  # maximum number of words to consider\n",
    "max_sequence_length = 100  # max length of each sequence (lyrics)\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Pad sequences to ensure uniform input size\n",
    "X_train = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "X_val = pad_sequences(val_sequences, maxlen=max_sequence_length)\n",
    "X_test = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Prepare genre labels (assuming they are categorical)\n",
    "y_train = pd.get_dummies(train_df['genre']).values\n",
    "y_val = pd.get_dummies(val_df['genre']).values\n",
    "y_test = pd.get_dummies(test_df['genre']).values\n",
    "\n",
    "max_artist_length = 1  # Since we are using one token for the artist\n",
    "\n",
    "# Tokenize the artist names\n",
    "artist_tokenizer = Tokenizer()\n",
    "artist_tokenizer.fit_on_texts(train_df['artist'].values)\n",
    "\n",
    "train_artist_sequences = artist_tokenizer.texts_to_sequences(train_df['artist'].values)\n",
    "val_artist_sequences = artist_tokenizer.texts_to_sequences(val_df['artist'].values)\n",
    "test_artist_sequences = artist_tokenizer.texts_to_sequences(test_df['artist'].values)\n",
    "\n",
    "# Pad sequences to ensure uniform input size for artist (usually 1 token per artist name)\n",
    "X_train_artist = pad_sequences(train_artist_sequences, maxlen=1)  # padding to length 1\n",
    "X_val_artist = pad_sequences(val_artist_sequences, maxlen=1)\n",
    "X_test_artist = pad_sequences(test_artist_sequences, maxlen=1)\n",
    "\n",
    "artist_vocab_size = len(artist_tokenizer.word_index) + 1  # Size of the artist vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "y_train_encoded = pd.get_dummies(train_df['genre'])\n",
    "\n",
    "# Save the one-hot encoded labels (y_train) to a file\n",
    "with open('y_train_encoded.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_encoded, f)\n",
    "\n",
    "    # Save the column names (genre classes) for consistency during testing\n",
    "with open('genre_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_encoded.columns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"X_test_artist shape: {X_test_artist.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout, LSTM\n",
    "\n",
    "def build_basic_rnn():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
    "    model.add(SimpleRNN(128, return_sequences=False))\n",
    "    model.add(Dropout(0.5))  # to prevent overfitting\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))  # multi-class classification\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "basic_rnn_model = build_basic_rnn()\n",
    "basic_rnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_rnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "lstm_model = build_lstm_model()\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multilayer_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "multi_layer_lstm_model = build_multilayer_lstm_model()\n",
    "multi_layer_lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_layer_lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the 50-dimensional embedding text file\n",
    "path = 'glove.6B.100d.txt'\n",
    "\n",
    "embeddings = {}\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open(path, 'r', encoding = 'utf-8') as f:\n",
    "    for line in f:\n",
    "      values = line.split()                                          # Each line in the file is a word + 50 integers denoting its vector.\n",
    "      embeddings[values[0]] = np.array(values[1:], 'float32')        # The first element of every line is a word & the rest 50 are its array of integers.\n",
    "\n",
    "\n",
    "# Building the embeddings matrix out of words present in our corpus\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < max_words:\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[index] = np.random.uniform(-0.1, 0.1, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multilayer_lstm_model_emb():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "multi_layer_lstm_model_emb = build_multilayer_lstm_model_emb()\n",
    "hidden1 = multi_layer_lstm_model_emb.layers[2]\n",
    "hidden1.name\n",
    "\n",
    "print(f\"hidden1 matrix shape: {hidden1.name}\")\n",
    "\n",
    "# multi_layer_lstm_model_emb.summary()\n",
    "\n",
    "print(f\"Embedding matrix shape: {multi_layer_lstm_model_emb.layers[0]}\")\n",
    "\n",
    "\n",
    "# Loading our pre-trained embedding matrix in the Embedding layer\n",
    "multi_layer_lstm_model_emb.layers[0].set_weights([embedding_matrix])\n",
    "multi_layer_lstm_model_emb.layers[0].trainable = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_basic_rnn_model_with_artist(max_lyrics_length, max_artist_length, max_words, artist_vocab_size, embedding_dim=128):\n",
    "    \"\"\"\n",
    "    Builds a basic RNN model that takes both lyrics and artist as inputs.\n",
    "    \n",
    "    Args:\n",
    "    - max_lyrics_length: The maximum length of the lyrics sequences.\n",
    "    - max_artist_length: The maximum length of the artist sequence (typically 1 token).\n",
    "    - max_words: The size of the vocabulary (number of unique words in lyrics).\n",
    "    - artist_vocab_size: The size of the artist vocabulary (number of unique artists).\n",
    "    - embedding_dim: The dimensionality of the embedding layer.\n",
    "    \n",
    "    Returns:\n",
    "    - model: The compiled Keras model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer for lyrics\n",
    "    input_lyrics = Input(shape=(max_lyrics_length,), name='lyrics_input')\n",
    "    # Embedding layer for lyrics\n",
    "    embedding_lyrics = Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_lyrics_length)(input_lyrics)\n",
    "    # RNN layer for lyrics\n",
    "    rnn_lyrics = SimpleRNN(128, activation='relu')(embedding_lyrics)\n",
    "    \n",
    "    # Input layer for artist\n",
    "    input_artist = Input(shape=(max_artist_length,), name='artist_input')\n",
    "    # Embedding layer for artist\n",
    "    embedding_artist = Embedding(input_dim=artist_vocab_size, output_dim=embedding_dim)(input_artist)\n",
    "    # Flatten the artist embedding to concatenate with the lyrics output\n",
    "    flatten_artist = Flatten()(embedding_artist)\n",
    "\n",
    "    # Combine both the RNN output (lyrics) and the artist embedding output\n",
    "    combined = tf.keras.layers.concatenate([rnn_lyrics, flatten_artist])\n",
    "\n",
    "    # Dense layers to predict the genre\n",
    "    x = Dense(128, activation='relu')(combined)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(10, activation='softmax')(x)  # 10 classes, softmax activation for multi-class\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=[input_lyrics, input_artist], outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "rnn_model_with_artist = build_basic_rnn_model_with_artist(max_sequence_length, max_artist_length, max_words, artist_vocab_size)\n",
    "\n",
    "# Display the model summary\n",
    "rnn_model_with_artist.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "\n",
    "def build_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_sequence_length))\n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "cnn_model = build_cnn_model()\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define the checkpoint callback dynamically using lambda to include the model name\n",
    "checkpoint_path = f'part_1_new/best_rnn_model.keras'\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Train Basic RNN Model\n",
    "history_basic_rnn = basic_rnn_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "basic_rnn_model.summary()\n",
    "\n",
    "# Load the best model saved during training\n",
    "best_model = load_model(f'part_1_new/best_rnn_model.keras')\n",
    "\n",
    "# Evaluate the model on the test data (Block 1)\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Store the results in the list\n",
    "results.append({\n",
    "    'model_name': 'best_rnn_model',\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'model_history': history_basic_rnn\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define the checkpoint callback dynamically using lambda to include the model name\n",
    "checkpoint_path = f'part_1_new/best_lstm_model.keras'\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Train LSTM Model\n",
    "history_lstm = lstm_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "lstm_model.summary()\n",
    "\n",
    "# Load the best model saved during training\n",
    "best_model = load_model(f'part_1_new/best_lstm_model.keras')\n",
    "\n",
    "# Evaluate the model on the test data (Block 1)\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Store the results in the list\n",
    "results.append({\n",
    "    'model_name': 'best_lstm_model',\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'model_history': history_lstm\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define the checkpoint callback dynamically using lambda to include the model name\n",
    "checkpoint_path = f'part_1_new/best_multi_layer_lstm_model.keras'\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Train Multi-layer LSTM Model\n",
    "history_multi_lstm = multi_layer_lstm_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "multi_layer_lstm_model.summary()\n",
    "\n",
    "# Load the best model saved during training\n",
    "best_model = load_model(f'part_1_new/best_multi_layer_lstm_model.keras')\n",
    "\n",
    "# Evaluate the model on the test data (Block 1)\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Store the results in the list\n",
    "results.append({\n",
    "    'model_name': 'best_multi_layer_lstm_model',\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'model_history': history_multi_lstm\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define early stopping to monitor the validation loss\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define the checkpoint callback dynamically to save the best model based on val_loss\n",
    "checkpoint_path = 'part_1_new/best_rnn_model_with_artist.keras'\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model with both lyrics and artist inputs, and validation data\n",
    "history_rnn_model_with_artist = rnn_model_with_artist.fit(\n",
    "    [X_train, X_train_artist],  # Inputs: lyrics and artist\n",
    "    y_train,  # Output: genre labels\n",
    "    epochs=10,  # Number of epochs to train\n",
    "    batch_size=64,  # Batch size\n",
    "    validation_data=([X_val, X_val_artist], y_val),  # Validation data: lyrics and artist\n",
    "    callbacks=[early_stopping, checkpoint]  # Callbacks for early stopping and model checkpointing\n",
    ")\n",
    "\n",
    "# Print the model summary to check the structure\n",
    "rnn_model_with_artist.summary()\n",
    "\n",
    "# Load the best model saved during training\n",
    "best_model = load_model('part_1_new/best_rnn_model_with_artist.keras')\n",
    "\n",
    "# Evaluate the model on the test data (Block 1)\n",
    "test_loss, test_acc = best_model.evaluate([X_test, X_test_artist], y_test)\n",
    "\n",
    "# Store the results in a results list (assuming results is predefined)\n",
    "results.append({\n",
    "    'model_name': 'best_rnn_model_with_artist',\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'model_history': history_rnn_model_with_artist\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "genre_labels = train_df['genre'].unique()  # Or test_df['genre'] if you want the test set\n",
    "\n",
    "best_model = load_model('part_1_new/best_rnn_model_with_artist.keras')\n",
    "\n",
    "# Step 1: Run predictions on the test data\n",
    "predictions = best_model.predict([X_test, X_test_artist])\n",
    "\n",
    "# Step 2: Convert predicted probabilities to class indices\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Step 3: Convert the true labels (y_test) from one-hot encoded format to class indices\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Step 4: Create confusion matrix\n",
    "cm = confusion_matrix(y_test_classes, predicted_classes)\n",
    "\n",
    "# Step 5: Plot the confusion matrix as a heatmap for better visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=genre_labels, yticklabels=genre_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Optionally, print classification report for more detailed performance metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, predicted_classes, target_names=genre_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Define the checkpoint callback dynamically using lambda to include the model name\n",
    "checkpoint_path = f'part_1_new/best_cnn_model.keras'\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Train CNN Model\n",
    "history_cnn = cnn_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2, callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "# Load the best model saved during training\n",
    "best_model = load_model(f'part_1_new/best_cnn_model.keras')\n",
    "\n",
    "# Evaluate the model on the test data (Block 1)\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test)\n",
    "\n",
    "# Store the results in the list\n",
    "results.append({\n",
    "    'model_name': 'best_cnn_model',\n",
    "    'test_loss': test_loss,\n",
    "    'test_accuracy': test_acc,\n",
    "    'model_history': history_cnn\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the results array to a file\n",
    "with open('results_v1_part1.pkl', 'wb') as file:\n",
    "    pickle.dump(results, file)\n",
    "print(\"Results array saved to 'results_v1_part1.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the results array from a file\n",
    "with open('results_v1_part1.pkl', 'rb') as file:\n",
    "    loaded_results = pickle.load(file)\n",
    "print(\"Results array loaded from 'results_v1_part1.pkl'.\")\n",
    "\n",
    "# Plot training and validation accuracy dynamically\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in loaded_results:\n",
    "    model_name = result['model_name']\n",
    "    history = result['model_history']\n",
    "    plt.plot(history.history['loss'], label=f'{model_name} Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label=f'{model_name} Validation Loss')\n",
    "\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the results array from a file\n",
    "with open('results_v1_part1.pkl', 'rb') as file:\n",
    "    loaded_results = pickle.load(file)\n",
    "print(\"Results array loaded from 'results_v1_part1.pkl'.\")\n",
    "\n",
    "\n",
    "# Print test accuracies dynamically\n",
    "for result in loaded_results:\n",
    "    print(f\"{result['model_name']} Test Accuracy: {result['test_accuracy']}\")\n",
    "\n",
    "# Plot training and validation accuracy dynamically\n",
    "plt.figure(figsize=(12, 6))\n",
    "for result in loaded_results:\n",
    "    model_name = result['model_name']\n",
    "    history = result['model_history']\n",
    "    plt.plot(history.history['accuracy'], label=f'{model_name} Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label=f'{model_name} Validation Accuracy')\n",
    "\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the results array from a file\n",
    "with open('results_v1_part1.pkl', 'rb') as file:\n",
    "    loaded_results = pickle.load(file)\n",
    "print(\"Results array loaded from 'results_v1_part1.pkl'.\")\n",
    "\n",
    "# Print test accuracies dynamically\n",
    "for result in loaded_results:\n",
    "    print(f\"{result['model_name']} Test Accuracy: {result['test_accuracy']}, Test Loss: {result['test_loss']}\")\n",
    "\n",
    "# Prepare data for bar plots\n",
    "model_names = [result['model_name'] for result in loaded_results]\n",
    "test_accuracies = [result['test_accuracy'] for result in loaded_results]\n",
    "test_losses = [result['test_loss'] for result in loaded_results]\n",
    "\n",
    "# Plot Test Accuracy Bar Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(model_names, test_accuracies, color='green')\n",
    "plt.title('Test Accuracy per Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate model names for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of x-axis labels\n",
    "plt.show()\n",
    "\n",
    "# Plot Test Loss Bar Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(model_names, test_losses, color='red')\n",
    "plt.title('Test Loss per Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate model names for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of x-axis labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "model_url = \"https://github.com/fizz3r/ml/raw/main/part_1_v2/best_rnn_model_with_artist_2.keras\"\n",
    "model_filename = \"best_rnn_model_with_artist_2.keras\"\n",
    "\n",
    "model_path = tf.keras.utils.get_file(model_filename, model_url)\n",
    "best_rnn_model_with_artist_2 = tf.keras.models.load_model(model_path)\n",
    "\n",
    "predictions = best_rnn_model_with_artist_2.predict([X_test, X_test_artist])\n",
    "# Convert the predicted probabilities to class indices\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Convert the true labels (y_test) to class indices\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test_classes, predicted_classes)\n",
    "\n",
    "print(f\"Prediction Accuracy: {accuracy}\")\n",
    "\n",
    "# Step 4: Evaluate the model on the test data\n",
    "test_loss, test_acc = best_rnn_model_with_artist_2.evaluate([X_test, X_test_artist], y_test)\n",
    "\n",
    "# Print the evaluation results (loss and accuracy)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming X_test, X_test_artist, and y_test are your test data\n",
    "\n",
    "# Save X_test to a pickle file\n",
    "with open('X_test.pkl', 'wb') as file:\n",
    "    pickle.dump(X_test, file)\n",
    "print(\"X_test saved to 'X_test.pkl'.\")\n",
    "\n",
    "# Save X_test_artist to a pickle file\n",
    "with open('X_test_artist.pkl', 'wb') as file:\n",
    "    pickle.dump(X_test_artist, file)\n",
    "print(\"X_test_artist saved to 'X_test_artist.pkl'.\")\n",
    "\n",
    "# Save y_test to a pickle file\n",
    "with open('y_test.pkl', 'wb') as file:\n",
    "    pickle.dump(y_test, file)\n",
    "print(\"y_test saved to 'y_test.pkl'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
